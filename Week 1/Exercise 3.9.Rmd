---
documentclass: article
geometry: margin=1in
fontsize: 11pt
classoption: a4paper
title: "MAS202M Applied Data Analysis — Exercise 3.9"
date: "\\today"
author: "Kári Hlynsson"
output: 
  bookdown::pdf_document2:
    keep_tex: yes
    fig_caption: true
    number_sections: false
    highlight: tango
    includes:
      in_header: ~/.local/share/rmd/preamble.tex
toc: false
lot: false
lof: false
---


In this exercise I will be looking into the `Auto` data set, the same one as in Exercise 2.8.
Start with the packages:

```{r message = FALSE, include = FALSE}
library(MASS)
library(tidyverse)
library(GGally)
library(ISLR2)
library(equatiomatic)

data(Auto)

panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    text(0.5, 0.5, txt, cex = 1.5)
}

pair_plot <- function(x) { pairs(x, diag.panel = panel.hist, upper.panel = panel.cor) }
```

Let's start with some routine exploration of the data. First of all, the data types:

```{r}
glimpse(Auto)
```

The data is 392 observations of 9 variables. Let's create a scatterplot matrix of the variables in the data set:

# Part (a)
Produce a scatterplot matrix which includes all of the variables in the data set.

```{r}
pair_plot(Auto |> select(-name))
```

# Part (b)
Correlation matrix:

```{r}
cor(Auto |> select(-name))
```

As I commented on previously, there are some quite strong correlations between some of the variables. For instance, `displacement` and
`horsepower` have a correlation coefficient of 0.9329944.

# Part (c)
> Use the `lm()` function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the `summary()` function to print the results. Comment on the output. For instance:

Start by fitting the model:

```{r}
model_all <- lm(mpg ~ . - name, data = Auto)

summary(model_all)
```

The linear model fitted to the data is specified by

```{r echo = FALSE}
extract_eq(model_all, use_coefs = TRUE, coef_digits = 4, wrap = TRUE)
```

Significant non-intercept coefficents include `displacement`, `weight`, `year` and `origin`.
The fact that the rest are not significant may be due to due to collinearity between variables (e.g. `weight` and `displacement`) as seen earlier in the correlation matrix from part (b). Thus we can not conclude that the coefficients for these variables are zero, but we will certainly investigate to see whether accounting for variable interactions changes our findings.


#### i. Is there a relationship between the predictors and the response?
Yes.

#### ii. Which predictors appear to have a statistically significant relationship to the response?
See above.

#### iii. What does the coefficient for the year variable suggest?
The coefficient for year is $0.750773$, which suggests a positive relationship between `year` and `mpg`, i.e. more recent vehicles correlate with higher fuel consumption per mile driven. A unit increase in `year` where other predictors is kept constant causes an increase of $0.750773$ in `mpg`.

# Part (d)
Create the diagnostic plots:

```{r}
par(mfrow = c(2, 2))
plot(model_all)
```

The residual structure (top left) seemingly suffers from non-constant variance (heteroscedasticity). This might indicate that our model assumptions are not sufficient.
The Residuals vs Leverage plot (bottom right) also shows a rather large observation with quite high leverage. Additionally, bla bla
