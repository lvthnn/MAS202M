---
title: "Week 1 — Exercise 2.9"
author: "Kári Hlynsson"
output: 
  bookdown::html_document2:
    toc: true
    toc_float:
      toc_collapsed: false    
      smooth_scroll: false
    number_sections: false
    toc_depth: 3
---



```{r message = FALSE, include = FALSE}
library(MASS)
library(ISLR2)
library(equatiomatic)

data(Auto)

panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    text(0.5, 0.5, txt, cex = 1.5)
}

pair_plot <- function(x) { pairs(x, diag.panel = panel.hist, upper.panel = panel.cor) }
```

# Part (a)
Produce a scatterplot matrix which includes all of the variables in the data set.

```{r}
pair_plot(Auto[,-c(8, 9)])
```

# Part (b)
Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the `name` variable
which is qualitative.

```{r}
cor(Auto[,-9])
```

# Part (c)
Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except `name`
as the predictors. Use the `summary()` function to print the results. Comment on the output. For instance.
- Is there a relationship between the predictors and the reponse?
- Which predictors appear to have a statistically significant relationship to the reponse?
- What does the coefficient for the `year` variable suggest?

```{r}
lin_all <- lm(mpg ~ . - name, data = Auto)
summary(lin_all)
```

The output suggests that there is indeed a relationship between a subset of predictors, since the $F$-test has p-value less than 0.05.
This tells us that for the covariate effects, $\beta_1 = \cdots = \beta_p = 0$ does not hold i.e. not all effects are truly zero.

Significant effects are `displacement`, `weight`, `year` and `origin`. However we might want to investigate interaction and see whether
collinearity may lead to unexpected results.

The coefficient for `year` is 0.750773, which means that if we took two identical observations where only the `year` variable differs by some
amount $x$, then we would expect the `mpg` between observations to differ by $0.750773 \cdot x$.

# Part (d)
Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit.
Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r}
par(mfrow = c(2, 2))
plot(lin_all)
```
 
The above plot indicates non-linear residual structure which may indicate that the model assumptions are not correct (i.e. linear relationship).
There is one observations which has very high leverage but does not have high error in the fit (not an outlier). Overall, there do not seem to be
very many outliers. We could test this further with a Jackknife test.

# Part (e)
Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

```{r}
lin_adj <- lm(mpg ~ weight*displacement + horsepower*displacement)
summary(lin_adj)
```

All effects are significant, and the interaction effect between `displacement` and `horsepower` is significant whereas `displacement` and `horsepower` is not.

```{r}
par(mfrow = c(2, 2))
plot(lin_adj)
```

This residual structure looks a bit more normal given this model although heteroscedacity is still seemingly present. We could also compare the effectiveness of the models:

```{r}
anova(lin_all, lin_adj)
```

The new model is clearly better.

# Part (f)
Try a few different transformations of the variables, such as $\log(X)$, $\sqrt{X}$ and $X^2$. Comment on your findings.

Let's try the Box-Cox transformation.

```{r}
bc <- boxcox(lin_adj)
lambda <- bc$x[which.max(bc$y)]
```

Box-Cox transformation suggests $\lambda = -0.3030303$. Let's transform the response, and remove the interaction effect
between `weight` and `displacement`.

```{r}
lin_adj <- update(lin_adj, mpg^lambda ~ . - weight:displacement)
summary(lin_adj)
```

The model is performing better, although it is more difficult to interpret the response :-(

```{r}
par(mfrow = c(2, 2))
plot(lin_adj)
```

Residual structure looks good.