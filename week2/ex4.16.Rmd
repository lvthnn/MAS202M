---
title: "Exercise 4.16"
author: "Kári Hlynsson"
date: "2024-01-21"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(e1071)
library(pROC)
library(class)
library(caret)

panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    text(0.5, 0.5, txt, cex = 1.5)
}

pair_plot <- function(x) { 
  pairs(x, diag.panel = panel.hist, upper.panel = panel.cor) 
}
```

Using the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your findings.

*Hint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set.*

```{r message = FALSE}
library(ISLR2)

data(Boston)
attach(Boston)
```

Start by creating the response variable. We will convert it to a factor at a later time.

```{r}
Boston$crim01 <- ifelse(crim > median(crim), 1, 0)
```

```{r}
plot(Boston, diag.panel = panel.hist, upper.panel = panel.cor, col = ifelse(Boston$crim01, 2, 4))
```

Let's get our hands dirty! Figure out which predictors are most correlated with the response:

```{r}
cor_mat <- cor(Boston)
crim_vec <- cor_mat[2:13,14]

crim_vec[order(abs(crim_vec), decreasing = TRUE)]
```

They are ($\rho > 0.5$)

-   `nox`: nitrogen oxides concentration (parts per 10 million).
-   `rad`: index of accessibility to radial highways.
-   `dis`: weighted mean of distances to five Boston employment centres.
-   `age`: proportion of owner-occupied units built prior to 1940.
-   `indus`: proportion of non-retail business acres per town.

Let's fit a logistic regression model to get a feel for what predictor subset we should use for the classifiers. I will pick the ones which achieve
statistical significance when fitted to the entire `Boston` data set.

```{r}
glm.fit.sub <- glm(crim01 ~ . - crim, family = "binomial", data = Boston)
summary(glm.fit.sub)
```
The significant variables are `zn`, `nox`, `dis`, `rad`, `tax`, `ptratio` and `medv`.

## Train / test split

Split the data immediately into training / test splits for validation of model accuracy. I will use 75% of the data for training:

```{r}
set.seed(1)

n <- nrow(Boston)
sel <- sample(1:n, round(0.75 * n))
col <- c(2, 5, 8, 9, 10, 11, 13, 14)

train <- Boston[sel, col]
train.X <- train[,-8]
train.Y <- train[,8]

test <- Boston[setdiff(1:n, sel), col]
test.X <- test[,-8]
test.Y <- test[,8]
```

## Logistic Regression

```{r}
glm.fit <- glm(crim01 ~ ., family = "binomial", data = train)
summary(glm.fit)
```

Find the optimal model using stepwise regression to get an idea of what subset of predictors is best:

```{r}
glm.fit <- stepAIC(glm.fit)
summary(glm.fit)
```

The optimal model specified by bidirectional stepwise AIC regression is

$$
\log\left[ \frac { \widehat{\Pr( \operatorname{crim01} = \operatorname{1} )} }{ 1 - \widehat{\Pr( \operatorname{crim01} = \operatorname{1} )} } \right] = -38.6401 - 0.0935(\operatorname{zn}) + 51.9146(\operatorname{nox}) + 0.8189(\operatorname{dis}) + 0.6974(\operatorname{rad}) - 0.0079(\operatorname{tax}) + 0.2783(\operatorname{ptratio}) + 0.085(\operatorname{medv})
$$

How does it perform?

```{r}
glm.probs <- predict(glm.fit, test.X, type = "response")
glm.preds <- ifelse(glm.probs >= 0.5, 1, 0)

confusionMatrix(factor(glm.preds), factor(test.Y))
```

The accuracy is 0.8889 with a sensitivity of 0.8710 and a specificity of 0.9062. All in all a decent model.

```{r}
par(pty = "s")
roc(test.Y, glm.probs, plot = TRUE, legacy.axes = TRUE)
```

## LDA

The assumptions posed by LDA are a linear decision boundary, so hopes are not exactly high here.

```{r}
lda.fit <- lda(crim01 ~ ., data = train)
lda.fit
```

Let's measure accuracy:

```{r}
lda.pred <- predict(lda.fit, test.X)$class
confusionMatrix(factor(lda.pred), factor(test.Y))
```

The LDA fit performs (better?) than logistic regression with an accuracy of 0.8492, a sensitivity of 0.9677 (very good) and a specificity of 0.7344 (not so good). Saying which is better depends on the context — which is better, sensitivity or specificity?

## Naive Bayes

```{r}
nb.fit <- naiveBayes(crim01 ~ ., data = train)
nb.fit
```

Measure accuracy:

```{r}
nb.pred <- predict(nb.fit, test.X)
confusionMatrix(factor(nb.pred), factor(test.Y))
```

Does not stand out too well compared to the other two.

## KNN

Let's optimise $k$ immediately.

```{r}
ks <- sapply(1:100, function(k) {
  accs <- c()
  for (i in 1:100) {
    knn.pred <- knn(train.X, test.X, train.Y, k = k)
    cfm <- table(knn.pred, test.Y)
    acc <- (cfm[1,1] + cfm[2,2]) / (sum(cfm))
    accs <- append(accs, acc)
  }
  return(mean(accs))
})

plot(ks, type = "b", xlab = "k", ylab = "Accuracy")
```

Seems like $k = 1$ is optimal.

```{r}
k_opt <- which.max(ks)
k_opt
```

Let's grab some more statistics.

```{r}
knn.pred <- knn(train.X, test.X, train.Y, k = k_opt)
confusionMatrix(factor(knn.pred), factor(test.Y))
```
kNN has an excellent accuracy of 0.9603 with sensitivity 0.9516 and specificity 0.9688 -- a great model! This is not surprising given that
we have $n \gg p$ in this context and we can not be certain that the decision boundary is linear. Clearly kNN is dominant, but we must make a
note of the fact that no crossvalidation was used so we might see some other trends depending on what data subset we use.