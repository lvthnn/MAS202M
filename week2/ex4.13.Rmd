---
title: "Week 2— Exercise 3.13"
author: "Kári Hlynsson"
output: 
  bookdown::html_document2:
    toc: true
    toc_float:
      toc_collapsed: false    
      smooth_scroll: false
    toc_depth: 3
---

```{r, include=FALSE}
panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    text(0.5, 0.5, txt, cex = 1.5)
}

pair_plot <- function(x) { pairs(x, diag.panel = panel.hist, upper.panel = panel.cor) }
```

```{r message = FALSE}
library(ISLR2)
library(MASS)
library(caret)
library(pROC)
library(class)
library(equatiomatic)

data(Weekly)
attach(Weekly)
```

# Part (a)
Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

```{r}
summary(Weekly)
```

Look for correlation between variables in data set.

```{r}
cor(Weekly[,-9])
```

There aren't any strikingly strong correlations between variables to be seen.
Visualise with improved pair plot to scan for correlations.

```{r message = FALSE}
pair_plot(Weekly)
```

All of the lag variables seem to be rather normally distributed as well as `Today`, and volume seems to be very left skewed. Let's
create some distribution diagrams.

```{r message = FALSE}
par(mfrow = c(2, 4))

for (i in 1:8) hist(Weekly[,i], main = NA, xlab = colnames(Weekly)[i])
```

There aren't really any observable relationships between variables.

## Part (b)
Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the `summary` function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data = Weekly)
```

```{r}
summary(glm.fits)
```
All of the non-intercept coefficients except `Lag2` fail to be significant in hypothesis tests.

## Part (c)
Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

Construct the confusion matrix:

```{r}
glm.probs <- predict(glm.fits, type = "response")

glm.pred <- rep("Down", nrow(Weekly))
glm.pred[glm.probs > 0.5] <- "Up"
glm.pred <- as.factor(glm.pred)

table(glm.pred, Weekly$Direction)
```
The accuracy of logistic regression as a classifier is given by

```{r}
sum(glm.pred == Weekly$Direction) / nrow(Weekly)
```

The accuracy is $0.5610652$, or approximately $56.11\%$. The confusion matrix we computed
tells us that 54 observations were correctly classified as `Down`, whereas 430 observations
were incorrectly classified as `Up`. The false positive rate (FPR, or sensitivity) is thus

```{r}
54 / (430 + 54)
```

or about $11\%$, not good. The true positive rate (TPR, specificity) is

```{r}
557 / (557 + 48)
```

or $92\%$, which is pretty good.

```{r}
confusionMatrix(glm.pred, Weekly$Direction)
```

## Part (d)
Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).
  
Split into test and training data:

```{r}
train <- Weekly[Year %in% 1990:2008,]
test  <- Weekly[!(Year %in% 1990:2008),]
```

Fit the logistic regression model:

```{r}
glm.fit <- glm(Direction ~ Lag2, family = binomial, data = train)
summary(glm.fit)
```

The fit is as follows:

```{r}
extract_eq(glm.fit)
```

Let's evaluate it on the test data set:

```{r}
glm.probs <- predict(glm.fit, test, type = "response")

glm.pred <- rep("Down", nrow(test))
glm.pred[glm.probs > 0.5] <- "Up"
glm.pred <- as.factor(glm.pred)
```

Construct the confusion matrix:

```{r}
confusionMatrix(glm.pred, test$Direction)
```

## Part (e)
Repeat (d) using LDA.

```{r}
lda.fit  <- lda(Direction ~ Lag2, data = train)
lda.pred <- predict(lda.fit, test)

confusionMatrix(lda.pred$class, test$Direction)
```

Identical to logistic regression classifier.

## Part (f)
Repeat (d) using QDA.

Fit:

```{r}
qda.fit  <- qda(Direction ~ Lag2, data = train)
qda.pred <- predict(qda.fit, test)

confusionMatrix(qda.pred$class, test$Direction)
```

Weird performance.

## Part (g)
Repeat (d) using KNN with \(K = 1\).

```{r}
set.seed(1)

train.X <- train[,-9]
train.Y <- train[, 9]

test.X <- test[,-9]

knn.pred <- knn(train.X, test.X, train.Y, k = 1)

confusionMatrix(knn.pred, test$Direction)
```

The KNN classifier is performing quite well!

## Part (h)
Repeat (d) using Naive Bayes.

```{r}
library(e1071)

nb.fit <- naiveBayes(Direction ~ Lag2, data = train)
nb.pred <- predict(nb.fit, test)

confusionMatrix(nb.pred, test$Direction)
```

Predicts up for everything — really bad performance.

## Part (i)
Which of these methods appears to provide the best results on this data?

KNN has the best accuracy.

## Part (j)
Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.

Vary \(K\) parameter for more accuracy. Find the "elbow" on a graph of \(K\) vs test set error to optimise bias-variance tradeoff.

```{r}
k.optim <- sapply(1:20, function(k) {
  l <- c()
  for (i in 1:1000) {
    knn.pred <- knn(train.X, test.X, train.Y, k = k)
    accuracy <- confusionMatrix(knn.pred, test$Direction)$overall[["Accuracy"]]
    l <- append(l, accuracy)
  }
  mean.accuracy <- mean(l)
})
```

```{r}
k.data <- data.frame(k.optim, k.val = 1:20)

plot(k.data$k.val, k.data$k.optim, xlab = "K", ylab = "Accuracy", type = "o")
```

Which value of \(K\) is performing best?

```{r}
k.data[order(k.data$k.optim, decreasing = TRUE),]
```

It seems like the best value is \(K = 11\).