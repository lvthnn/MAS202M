---
title: "Exercise 6.10"
author: "KÃ¡ri Hlynsson"
date: "2024-02-13"
output: html_document
---

We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will not explore this in a simulated data set.

## Part (a)

Generate a data set with $p = 20$ features, $n = 1000$ observations, and an associated quantitative response vector generated according to the model

$$
Y = X\beta + \epsilon
$$

where $\beta$ has some elements that are exactly equal to zero.

```{r}
library(Matrix)
set.seed(42)

means <- rnorm(20, mean = 0, sd = 20)
sds   <- runif(20, 5, 25)

X <- lapply(1:20, function(j) Xj <- rnorm(1000, means[j], sds[j]))
X <- as.matrix(do.call(cbind, X))
colnames(X) <- paste0("X", 1:20)

beta <- as.matrix(rsparsematrix(20, 1, nnz = 20 * runif(1, 0.75, 1)))

e <- rnorm(1000)

Y <- -5 + X %*% beta

data <- data.frame(X, Y)
```

Majestic ahh pair plot:

```{r}
par(mfrow = c(4, 5))
for (i in 1:20) {
  plot(X[,i], Y, xlab = paste0("X", i))
}
```

## Part (b)

Split your data into a training set containing $100$ observations and a test set containing $900$ observations.

```{r}
n <- 1000
s <- sample(n, 100)
t <- setdiff(1:n, s)

train <- data[s,]
test  <- data[t,]
```

## Part (c)

Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.

```{r}
library(leaps)

regfit.full <- regsubsets(Y ~ ., data = train, nvmax = 20)
reg.summary <- summary(regfit.full)
reg.summary
```

Visualise:

```{r}
train.mses <- sapply(1:20, function(i) {
  coefs <- coef(regfit.full, id = i)
  vars  <- which(colnames(X) %in% names(coefs))
  
  # predict Y
  X.pred    <- cbind(1, X[s,vars])
  beta.pred <- matrix(c(coefs[1], beta[vars,]), nrow = length(coefs))
  Y.pred    <- X.pred %*% beta.pred
  
  # MSE
  mean((train$Y - Y.pred)^2)
})

plot(train.mses, xlab = "Model size", ylab = "MSE", type = "b")
points(which.min(train.mses), min(train.mses), col = "red", cex = 2, lwd = 1.5)
```

## Part (d)

Plot the test set MSE with the best model of each size. For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimised for an intermediate model size.

## Part (e)

```{r}
test.mses <- sapply(1:20, function(i) {
  coefs <- coef(regfit.full, id = i)
  vars  <- which(colnames(X) %in% names(coefs))
  
  # predict Y-
  X.pred    <- cbind(1, X[t,vars])
  beta.pred <- matrix(c(coefs[1], beta[vars,]), nrow = length(coefs))
  Y.pred    <- X.pred %*% beta.pred
  
  # MSE
  mean((test[, "Y"] - Y.pred)^2)
})

plot(test.mses, type = "b", xlab = "Model size", ylab = "MSE")
points(which.min(test.mses), min(test.mses), col = "red", cex = 2, lwd = 1.5)
```

Compare the two on the same plot:

```{r}
plot(test.mses, type = "b", xlab = "Model size", ylab = "MSE", col = 4, lwd = 1.5)
lines(train.mses, xlab = "Model size", ylab = "MSE", type = "b", col = 7, lwd = 1.5)
legend("topright", bty = "n", legend = c("Training set", "Test set"), col = c(4, 7), lty = 1, pch = 21)
points(which.min(test.mses), min(test.mses), col = "red", cex = 2, lwd = 1.5)
```

## Part (f)

How does the model at which the test set MSE in minimised compare to the true model used to generate the data? Comment on the coefficient values.

```{r}
opt.model <- coef(regfit.full, id = 15)
opt.model
```

```{r}
as.vector(beta)
```

After staring at this for a bit one can see that the model retrieves the true model quite accurately. Hooray.

## Part (g)

Create a plot displaying $\sqrt{\sum_{j = 1}^p \left(\beta_j - \hat\beta_j^r\right)2}$ for a range of values of $r$, where $\hat\beta_j^r$ is the $j$th coefficient estimate for the best model containing $r$ coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?

```{r}
coefs <- lapply(1:20, function(i) coef(regfit.full, id = i)[-1])
beta  <- setNames(as.vector(beta), paste0("X", 1:20))

cmp <- sapply(coefs, function(c) {
  beta.sub <- beta[names(beta) %in% names(c)]
  sqrt(sum(beta.sub - c)^2)
})
```

```{r}
plot(cmp, type = "b", xlab = "Model size", ylab = "L1 norm")
```

The produced plot has a similar tail of decreasing L1 norm as the model size increase but clearly the model that minimises this quantity is the model with size $n = 16$, which is different from our previous results.
