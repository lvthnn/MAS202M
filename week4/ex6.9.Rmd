---
title: "Exercise 6.9"
author: "KÃ¡ri Hlynsson"
date: "2024-02-12"
output: html_document
---

In this exercise, we will predict the number of applications received using the other variables in the `College` data set.

## Part (a)
Split the data into a training set and a test set.

```{r}
library(ISLR2)
data(College)

set.seed(1)

rownames(College) <- NULL # clutter

n <- nrow(College)
s <- sample(c(TRUE, FALSE), n, replace = T, prob = c(0.75, 0.25))

# for lm
train <- College[s, ]
test  <- College[!s, ]

# for glmnet
train.X <- model.matrix(Apps ~ ., train)[,-1]
train.Y <- train$Apps

test.X <- model.matrix(Apps ~ ., test)[,-1]
test.Y <- test$Apps
```

## Part (b)
Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
lm.fit <- lm(Apps ~ ., data = train)
summary(lm.fit)
```
Test error:

```{r}
lm.pred <- predict(lm.fit, test)
mean((test.Y - lm.pred)^2)
```

## Part (c)
Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
library(glmnet)

# grid for the lambdas (glmnet)
grid <- 10^seq(10, -2, length = 100)

ridge.fit <- glmnet(train.X, train.Y, alpha = 0, lambda = grid, thresh = 1e-12)
plot(ridge.fit)
```

Cross validate to select best value of $\lambda$:

```{r}
cv.out.ridge <- cv.glmnet(train.X, train.Y, alpha = 0, lambda = grid, thresh = 1e-12)
plot(cv.out.ridge)
```

Best value is

```{r}
lambda.ridge <- cv.out.ridge$lambda.min
lambda.ridge
```

```{r}
ridge.pred <- predict(ridge.fit, s = lambda.ridge, newx = test.X)
mean((test.Y - ridge.pred)^2)
```

Similar to OLS. Coefs:

```{r}
predict(ridge.fit, s = lambda.ridge, type = "coefficient")
```

## Part (d)
Fit a ridge lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with
the number of non-zero coefficient estimates.

```{r}
lasso.fit <- glmnet(train.X, train.Y, alpha = 1, lambda = grid, thresh = 1e-12)
plot(lasso.fit)
```

Find best $\lambda$:

```{r}
cv.out.lasso <- cv.glmnet(train.X, train.Y, alpha = 1, lambda = grid, thresh = 1e-12)
plot(cv.out.lasso)
```

Optimal $\lambda$:

```{r}
lambda.lasso <- cv.out.lasso$lambda.min
lambda.lasso
```

Fit the model:

```{r}
lasso.pred <- predict(lasso.fit, s = lambda.lasso, test.X)
mean((test.Y - lasso.pred)^2)
```

Coefficients:

```{r}
predict(lasso.fit, s = lambda.lasso, type = "coefficient")
```

15 non-zero components. Similar performance to other models.

## Part (e)
Fit a PCR model on the training set, with $M$ chosen by cross-validation. Report the test error obtained, along with the value of $M$
selected by cross-validation.

```{r}
library(pls)

pcr.fit <- pcr(Apps ~ ., data = train, scale = TRUE, validation = "CV")
summary(pcr.fit)
```

Cool plot:

```{r}
validationplot(pcr.fit, val.type = "MSEP")
```


The CV error is lowest for 17 components, which is essentially least squares. Let's use that.

```{r}
pcr.pred <- predict(pcr.fit, ncomp = 17, test.X)
mean((test.Y - pcr.pred)^2)
```

Similar performance.

## Part (f)
Fit a PLS model on the training set, with $M$ chosen by cross-validation. Report the test error obtained, along with the value of $M$ selected
by cross-validation.

```{r}
pls.fit <- plsr(Apps ~ ., data = train, scale = TRUE, validation = "CV")
summary(pls.fit)
```

Cool plot time:

```{r}
validationplot(pls.fit, val.type = "MSEP")
```

Best (adjusted) CV error for 8 components, which reduces dimensionality of data. Let's try that.

```{r}
pls.pred <- predict(pls.fit, ncomp = 8, test.X)
mean((test.Y - pls.pred)^2)
```

Similar performance...

## Part (g)
Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test
errors resulting from these five approaches?

We will use the formula for $R^2$:

$$
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\sum_{i = 1}^n (y_i - \hat y_i)^2}{\sum_{i = 1}^n (y_i - \overline y)}.
$$

Tally ho:

```{r}
tss   <- sum((test.Y - mean(test.Y))^2) 
preds <- list(lm.pred, ridge.pred, lasso.pred, pcr.pred, pls.pred)
rsss  <- lapply(preds, function(v) sum((v - test.Y)^2))
rsqs  <- sapply(rsss, function(rss) 1 - rss/tss)
names <- c("OLS", "Ridge", "Lasso", "PCR", "PLS")

barplot(rsqs, names.arg = names)
```

Differences are barely noticable. However notice that OLS and PCR have the same $R^2$ which is not surprising since we used 17 components for PCR
which is equivalent to performing OLS. PLS did the best job of dimensionality reduction, with 8 dimensions, followed by lasso with 15.