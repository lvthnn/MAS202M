---
title: "Exercise 5.5"
author: "Kári Hlynsson"
date: "2024-01-21"
output: html_document
---

In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

## Part (a)

Use the `rnorm` function to generate a predictor $X$ of length $n = 100$, as well as a noise vector $\epsilon$ of length $n = 100$.

```{r}
set.seed(69)
n <- 100
x <- rnorm(n)
e <- rnorm(n)
```

## Part (b)

Generate a response vector $Y$ of length $n = 100$ according to the model

$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon,
$$

where $\beta_1, \beta_2, \beta_3$ and $\beta_3$ are constants of your choice.

```{r}
y <- 2 + 3 * x + 4 * x^2 + 5 * x^3 + e
ord <- order(x)

plot(x, y)
lines(x[ord], y[ord] - e[ord], type = "l", lwd = 2.5, xlab = "x", ylab = "y")
```

## Part (c)

Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors $X, X^2, \ldots, X^{10}$. What is $C_p$, $\mathrm{BIC}$ and adjusted $R^2$? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the `data.frame()` function to create a single data set containing both $X$ and $Y$.

```{r}
library(leaps)
df <- data.frame(x, y)

regfit.full <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5)
                          + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), 
                          data = df, nvmax = 10)
reg.summary <- summary(regfit.full)

par(mfrow = c(1, 3))
plot(1:10, reg.summary$bic, xlab = "Subset no.", ylab = "BIC", type = "b")
plot(1:10, reg.summary$adjr2, xlab = "Subset no.", ylab = expression('adj. R'^2), type = "b")
plot(1:10, reg.summary$cp, xlab = "Subset no.", ylab = expression(C[p]), type = "b")
```

Prediction curves for each of the models.

```{r}
px <- lapply(1:10, function(d) x^d)
px <- do.call(cbind, px)
px <- data.frame(px)
colnames(px) <- c("x", paste0("x", 2:10))

vars_mdl <- reg.summary$which[,-1]
vars_mdl <- unname(vars_mdl)

plot(x, y, col = 1, xlab = "X", ylab = "Y")
for (m in 1:10) {
  vars_m <- vars_mdl[m,]
  data_m <- data.frame(cbind(px[,vars_m], y))
  
  model.fit  <- lm(y ~ ., data = data_m)
  model.pred <- predict(model.fit, data_m)
  
  lines(x[ord], model.pred[ord], lty = m, col = m, lwd = 1.25)
}

legend("topleft", inset = c(0.05, 0.05), legend = paste("Subset", 1:10), cex = 0.55, bty = "n",
       col = 1:10, lty = 1:10, lwd = 1.25)
```

Let's take a look at which model has the best measure of performance. From the visualisations we would expect it to be no. 3:

```{r}
which.max(reg.summary$adjr2); which.min(reg.summary$cp); which.min(reg.summary$bic)
```

The third model is the most dominant, and clearly there is little benefit in adding more complex terms to the model at the cost of increased variability. The fit involving the following data

```{r}
x.opt <- px[,vars_mdl[3,]]
head(x.opt)
```

Our original data! Make the fit:

```{r}
data.opt <- data.frame(x.opt, y)
opt.fit <- lm(y ~ ., data = data.opt)
summary(opt.fit)
```

Estimates are quite close to our original fit.

```{r}
pred.opt <- predict(opt.fit, data.opt)

plot(x, y)
lines(x[ord], y[ord] - e[ord], type = "l", lwd = 2.5, xlab = "X", ylab = "Y", col = 3)
lines(x[ord], pred.opt[ord], type = "l", lwd = 2.5, col = 4)
```

## Part (d)

Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?

```{r}
library(MASS)
model.full <- lm(y ~ ., data = px)
model.null <- lm(y ~ 1, data = px)

model.back <- stepAIC(model.full, direction = "backward", trace = FALSE)
model.forw <- stepAIC(model.null, direction = "forward", scope = list(lower = model.null, upper = model.full), trace = FALSE)

summary(model.back)
summary(model.forw)
```

Same models again.

## Part (e)

Now fit a lasso model to the simulate data, again using $X, X^2, \ldots, X^{10}$ as predictors. Use cross-validation to select the optimal value of $\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.

Let's take a look at how different values of $\lambda$ affect the coefficients obtained through lasso.

```{r}
library(glmnet)

lasso.mod <- glmnet(px, y, alpha = 1)
plot(lasso.mod)
```

Picking $\lambda$ small enough assigns some of the coefficients to zero. Which value of $\lambda$ is best?

```{r}
lasso.res <- cv.glmnet(as.matrix(px), y, alpha = 1)
lambda.opt <- lasso.res$lambda.min
lambda.opt
```

The code indicates that $\lambda = 0.07493406$ is optimal. We will use this to fit the lasso model.

```{r}
model.lasso <- predict(lasso.mod, s = lambda.opt, type = "coefficient")
model.lasso
```

The model fit by lasso is very similar to the models attained using other methods – however the quartic term $X^4$ is included in the model with a very small coefficient.

Plot the optimal model from the subset selection and the lasso model:

```{r}
par(mfrow = c(1,2))
plot(x, y, xlab = "X", ylab = "Y")
lines(x[ord], pred.opt[ord], type = "l", lwd = 2.5, col = 2)

plot(x, y, xlab = "X", ylab = "Y")
lines(x[ord], lasso.pred[ord], type = "l", lwd = 2.5, col = 4)
```

The fits are very similar.

## Part (f)

Now generate a response vector $Y$ according to the model

$$
Y = \beta_0 + \beta_7 X^7 + \epsilon,
$$

and perform best subset selection and the lasso. Discuss the results obtained.

```{r}
y <- 2 + 6 * x^7 + e
plot(x, y)
```

Best subset selection:

```{r}
df <- data.frame(x, y)

regfit.full <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5)
                          + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), 
                          data = df, nvmax = 10)
reg.summary <- summary(regfit.full)

par(mfrow = c(1, 3))
plot(1:10, reg.summary$bic, xlab = "Subset no.", ylab = "BIC", type = "b")
plot(1:10, reg.summary$adjr2, xlab = "Subset no.", ylab = expression('adj. R'^2), type = "b")
plot(1:10, reg.summary$cp, xlab = "Subset no.", ylab = expression(C[p]), type = "b")
```

```{r}
px <- lapply(1:10, function(d) x^d)
px <- do.call(cbind, px)
px <- data.frame(px)
colnames(px) <- c("x", paste0("x", 2:10))

vars_mdl <- reg.summary$which[,-1]
vars_mdl <- unname(vars_mdl)

plot(x, y, col = 1, xlab = "X", ylab = "Y")
for (m in 1:10) {
  vars_m <- vars_mdl[m,]
  data_m <- data.frame(cbind(px[,vars_m], y))
  
  model.fit  <- lm(y ~ ., data = data_m)
  model.pred <- predict(model.fit, data_m)
  
  lines(x[ord], model.pred[ord], lty = m, col = m, lwd = 1.25)
}

legend("topleft", inset = c(0.05, 0.05), legend = paste("Subset", 1:10), cex = 0.55, bty = "n",
       col = 1:10, lty = 1:10, lwd = 1.25)
```

```{r}
which.max(reg.summary$adjr2); which.min(reg.summary$cp); which.min(reg.summary$bic)
```

```{r}
x.opt <- px[,vars_mdl[1,]]
head(x.opt)
```

```{r}
data.opt <- data.frame(x.opt, y)
opt.fit <- lm(y ~ ., data = data.opt)
summary(opt.fit)
```

Hmm. Looks close to our original fit! Red is true model and blue is model recovered by best subset.

```{r}
pred.opt <- predict(opt.fit, data.opt)

par(mfrow = c(1, 2))
plot(x, y, xlab = "X", ylab = "Y")
lines(x[ord], y[ord] - e[ord], type = "l", lwd = 2.5, xlab = "X", ylab = "Y", col = 2)
plot(x, y, xlab = "X", ylab = "Y")
lines(x[ord], pred.opt[ord], type = "l", lwd = 2.5, col = 4)
```

Lasso:

```{r}
lasso.init <- glmnet(px, y, alpha = 1)
plot(lasso.init)
```

```{r}
lasso.res <- cv.glmnet(as.matrix(px), y, alpha = 1)
lambda.opt <- lasso.res$lambda.min
lambda.opt
```

```{r}
lambda.mod <- predict(lasso.init, s = lambda.opt, type = "coefficients")
lambda.mod
```

Lasso recovers the model almost exactly.
