---
title: "Exercise 5.5"
author: "KÃ¡ri Hlynsson"
date: "2024-01-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

In Chapter 4, we used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

```{r message = FALSE}
library(ISLR2)
library(caret)
library(tidyverse)

data(Default)
```

## Part (a)

Fit a logistic regression model that uses income and balance to predict default.

```{r}
glm.fit <- glm(default ~ income + balance, data = Default, family = "binomial")
```

## Part (b)

Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:

1.  Split the sample into a training set and a validation set.

2.  Fit a multiple logistic regression model using only the training observations.

3.  Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the `default` category if the posterior probability is greater than 0.5.

4.  Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified

Perform the training-test split. I will commit 75% of the data set to the training set and use the rest for measuring prediction error.

```{r}
set.seed(42)

n   <- nrow(Default)
sel <- sample(n, round(0.75 * n))

train <- Default[sel,]
test  <- Default[setdiff(1:n, sel),]
```

Fit the model on the training set:

```{r}
glm.fit <- glm(default ~ income + balance, data = train, family = "binomial")
```

Now create predictions on the test set and measure prediction error on the test data set:

```{r}
test.probs <- predict(glm.fit, test, type = "response")
test.pred  <- factor(ifelse(test.probs > 0.5, "Yes", "No"))

confusionMatrix(test.pred, test$default, positive = "Yes")
```

The validation set error (test error) is $1 - 0.9748 = 0.0252$ or approximately $2.52\%$.

## Part (c)

Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.

We will perform a $k$-fold cross validation with $k = 3$. Start by creating the folds:

```{r}
k <- 3
n <- nrow(train)
folds <- sample(k, n, replace = TRUE)
prop.table(table(folds)) # approximately equal sizes
```

Now for the fitting procedure:

```{r}
errs <- sapply(1:k, function(i) {
  train_i <- train[!(folds == i),]
  test_i  <- test[folds == i,]
  glm.fit <- glm(default ~ income + balance, data = train_i, family = "binomial")
  
  # get measure of accuracy
  glm.prob <- predict(glm.fit, test_i[,-1], type = "response")
  glm.pred <- ifelse(glm.prob > 0.5, "Yes", "No")
  
  cnf.mat <- table(glm.pred, test_i$default)
  err_i <- (cnf.mat[1, 2] + cnf.mat[2, 1]) / sum(cnf.mat)
})
```

```{r}
mean(errs)
sd(errs)
```

The mean error from the $k$-fold CV with $k = 3$ is 0.02569011 and a standard deviation of 0.004443395. We could try seeing how this all changes with different values of $k$.

```{r}
seq_k <- round(seq(2, 7500, length = 100))
errs_ks <- lapply(seq_k, function(k) {
  folds <- sample(1:k, n, replace = (k < n))
  errs <- sapply(1:k, function(i) {
    train_i <- train[!(folds == i),]
    test_i  <- test[folds == i,]
    glm.fit <- glm(default ~ income + balance, data = train_i, family = "binomial")
    
    # get measure of accuracy
    glm.prob <- predict(glm.fit, test_i[,-1], type = "response")
    glm.pred <- factor(ifelse(glm.prob > 0.5, "Yes", "No"))
    
    cnf.mat <- table(glm.pred, test_i$default)
    err_i <- (cnf.mat[1, 2] + cnf.mat[2, 1]) / sum(cnf.mat)
  })
})
```

## Part (d)

Now consider a logistic regression model that predicts the probability of default using `income`, `balance`, and a dummy variable for `student`. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate.

```{r}
tse2 <- sapply(etas, function(eta) {
  accuracy <- c()
  
  for (i in 1:10) {
    sel <- sample(1:n, round(eta * n))
    
    train <- Default[sel,]
    test  <- Default[setdiff(1:n, sel),]
    
    mdl.fit    <- glm(default ~ balance + income + student, data = train, family = "binomial")
    test.probs <- predict(mdl.fit, test, type = "response")
    test.pred  <- factor(ifelse(test.probs > 0.5, "Yes", "No"), levels = c("No", "Yes"))
    
    accuracy <- append(accuracy, mean(test.pred == test$default))
  }
    
  return(mean(accuracy))
})
```

Compare the two

```{r}
plot(etas, 1 - tse1, type = "l", lwd = 1.15, col = "red",
     xlab = expression(eta), ylab = "Test set error", ylim = c(0, max(1 - tse1, 1 - tse2)))
lines(etas, 1 - tse2, lty = 1, lwd = 1.15, col = "blue")

legend("bottomleft", inset = c(0.05, 0.05), legend = c("No dummy var.", "With dummy var."),
       lwd = 1.5, lty = 1, col = c("red", "blue"))
```

Similar performance at the cost of an added variable. Adding the dummy variable comes at a cost of increased bias while not reducing variance -- therefore we should not include it.
